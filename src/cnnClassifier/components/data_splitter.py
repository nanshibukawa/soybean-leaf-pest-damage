import os
# üîá SILENCIA LOGS VERBOSOS DO TENSORFLOW 0=all, 1=info, 2=warning, 3=error
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Silencia logs verbosos
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'   # Remove warnings oneDNN
os.environ['CUDA_VISIBLE_DEVICES'] = '0' 



from dataclasses import dataclass
from pathlib import Path
import tensorflow as tf

from cnnClassifier.config.constants import DATA_SOURCE_DIR
from cnnClassifier.entity.config_entity import (
    DataSplitterConfig, 
    ImageConfig, 
    DataSubsetType)
from cnnClassifier.utils.logger import configure_logger

logger = configure_logger(__name__)

@dataclass
class DataSplitter:
    """Divide dados de imagem em conjuntos de treino e valida√ß√£o.
    
    Atributos:
        data_split_config (DataSplitterConfig): Configura√ß√£o para propor√ß√µes de divis√£o 
            de dados, tamanho do batch e random seed para divis√µes reproduz√≠veis
        image_config (ImageConfig): Configura√ß√£o para dimens√µes de imagem (altura/largura)
            e o caminho do diret√≥rio de dados
        subset (DataSubsetType): Defini√ß√µes de tipo para subconjuntos de dados (TRAIN/VALIDATION)
            usado para especificar qual por√ß√£o dos dados carregar
    """
    data_split_config: DataSplitterConfig
    image_config: ImageConfig
    subset: DataSubsetType

    def load_train_data(self):
        """
        Carrega dados de treino do diret√≥rio configurado.
        
        Cria um dataset TensorFlow para treino carregando imagens do diret√≥rio especificado
        e aplicando a propor√ß√£o de divis√£o de treino configurada. O dataset √© automaticamente
        embaralhado e agrupado em lotes de acordo com a configura√ß√£o.
        
        Returns:
            tf.data.Dataset: Dataset de treino contendo imagens agrupadas em batchs e
                pr√©-processadas com seus r√≥tulos correspondentes. Cada batch cont√©m imagens
                redimensionadas para as dimens√µes configuradas (altura x largura) e o
                dataset usa o subconjunto de treino baseado na propor√ß√£o de divis√£o.
        """
        treino = tf.keras.utils.image_dataset_from_directory(
            directory=self.image_config.data_dir,
            validation_split=1 - self.data_split_config.train_ratio,
            shuffle=True,
            subset=self.subset.TRAIN.value,
            seed=self.data_split_config.random_seed,
            image_size=(self.image_config.altura, self.image_config.largura),
            batch_size=self.data_split_config.batch_size
        )
        return treino

    def load_validation_data(self):
        """Carrega dados de valida√ß√£o do diret√≥rio configurado.
        
        Cria um dataset TensorFlow para valida√ß√£o carregando imagens do diret√≥rio 
        especificado e aplicando a propor√ß√£o de divis√£o de valida√ß√£o configurada. 
        O dataset √© automaticamente embaralhado e agrupado em batchs de acordo com a 
        configura√ß√£o, usando a mesma semente dos dados de treino para divis√µes consistentes.
        
        Returns:
            tf.data.Dataset: Dataset de valida√ß√£o contendo imagens agrupadas em batchs e
                pr√©-processadas com seus r√≥tulos correspondentes. Cada batch cont√©m imagens
                redimensionadas para as dimens√µes configuradas (altura x largura) e o
                dataset usa o subconjunto de valida√ß√£o baseado na propor√ß√£o de divis√£o.
        """
        validation = tf.keras.utils.image_dataset_from_directory(
            directory=self.image_config.data_dir,
            validation_split=self.data_split_config.val_ratio,
            shuffle=True,
            subset=self.subset.VALIDATION.value,
            seed=self.data_split_config.random_seed,
            image_size=(self.image_config.altura, self.image_config.largura),
            batch_size=self.data_split_config.batch_size,

        )
        return validation

if __name__ == "__main__":
    logger.info("Iniciando o DataSplitter...")
    
    data_split_config = DataSplitterConfig()
    image_config = ImageConfig(data_dir=Path(DATA_SOURCE_DIR))
    
    data_splitter = DataSplitter(
        data_split_config=data_split_config,
        image_config=image_config,
        subset=DataSubsetType
    )
    
    train_data = data_splitter.load_train_data()
    validation_data = data_splitter.load_validation_data()

    logger.info("Dados de treino e valida√ß√£o carregados com sucesso.")